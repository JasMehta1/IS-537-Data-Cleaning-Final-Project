{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Course Scrapper Notebook"
      ],
      "metadata": {
        "id": "qjnDJkNvMsPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfTqGDk_MfOq",
        "outputId": "574a2642-5c61-4df5-f317-34ab266a3aa9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.32.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.4.0)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.11/dist-packages (from selenium) (4.13.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (25.3.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n",
            "Downloading selenium-4.32.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: wsproto, outcome, trio, trio-websocket, selenium\n",
            "Successfully installed outcome-1.3.0.post0 selenium-4.32.0 trio-0.30.0 trio-websocket-0.12.2 wsproto-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ---\n",
        "# title: UIUC Course Catalog Scraper\n",
        "# author: Jas Mehta, Moksha Shah\n",
        "# date: 2025-05-11\n",
        "# colab-type: code\n",
        "# description: This notebook scrapes course data for all terms and departments from the University of Illinois Urbana-Champaign (UIUC) course schedule website (https://courses.illinois.edu).\n",
        "\n",
        "## 📘 UIUC Course Catalog Scraper\n",
        "\n",
        "This notebook automates the extraction of course listings from the [UIUC Course Explorer](https://courses.illinois.edu/schedule). It performs the following tasks:\n",
        "\n",
        "- Retrieves all academic terms (e.g., Fall 2025, Spring 2025)\n",
        "- Extracts all department codes (e.g., CS, MATH, ECE) for each term\n",
        "- Visits every course listing page (e.g., CS 225) and scrapes:\n",
        "    - Course title\n",
        "    - Section info\n",
        "    - Schedule\n",
        "    - Instructor\n",
        "    - CRN\n",
        "    - etc\n",
        "- Aggregates all results into a single pandas DataFrame\n",
        "- Saves the output as a CSV file\n",
        "\n",
        "### 🔧 Requirements\n",
        "\n",
        "- `requests`\n",
        "- `beautifulsoup4`\n",
        "- `pandas`\n",
        "- `selenium` with a supported driver (e.g., ChromeDriver)\n",
        "- `tqdm` (optional, for progress bars)\n",
        "\n",
        "> ⚠️ Note: Running this on Google Colab requires setting up Selenium with a headless browser environment, which may be complex. For best results, run this notebook locally with a full desktop browser.\n",
        "\n",
        "### ✅ Output\n",
        "\n",
        "- `uiuc_all_courses.csv`: Contains all course listings for all selected terms and departments.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S0VI-NoMMtXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "def extract_uiuc_course_urls(year: int, term: str, *, timeout: int = 15) -> list[str]:\n",
        "    \"\"\"\n",
        "    Return every course-level Course Explorer URL for the given UIUC term.\n",
        "\n",
        "    >>> extract_uiuc_course_urls(2025, \"fall\")[0]\n",
        "    'https://courses.illinois.edu/schedule/2025/fall/AAS/100'\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    year     : 4-digit academic year (e.g. 2025)\n",
        "    term     : 'spring', 'summer', or 'fall'  (case-insensitive)\n",
        "    timeout  : per-request timeout in seconds (default 15)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list[str] – sorted list of https://courses.illinois.edu/schedule/… URLs\n",
        "    \"\"\"\n",
        "    term = term.lower()\n",
        "    if term not in {\"spring\", \"summer\", \"fall\"}:\n",
        "        raise ValueError(\"term must be 'spring', 'summer', or 'fall'\")\n",
        "\n",
        "    # Root feed that lists every subject offered that term\n",
        "    root_url = (\n",
        "        f\"https://courses.illinois.edu/cisapp/explorer/schedule/{year}/{term}.xml\"\n",
        "    )\n",
        "\n",
        "    sess = requests.Session()\n",
        "    sess.headers[\"User-Agent\"] = \"UIUC-course-scraper/1.0 (+github.com/you)\"\n",
        "\n",
        "    def _get_xml(url: str) -> ET.Element:\n",
        "        \"\"\"Download *url* and return its root Element.\"\"\"\n",
        "        r = sess.get(url, timeout=timeout)\n",
        "        r.raise_for_status()\n",
        "        return ET.fromstring(r.content)\n",
        "\n",
        "    course_urls: list[str] = []\n",
        "\n",
        "    # 1) Subject listing -------------------------------------------------------\n",
        "    for subj in _get_xml(root_url).iter(\"subject\"):\n",
        "        # Each <subject … href=\"…/CS.xml\"> element already contains the API link\n",
        "        subj_xml = subj.attrib[\"href\"]\n",
        "        # 2) Course listing inside that subject --------------------------------\n",
        "        for course in _get_xml(subj_xml).iter(\"course\"):\n",
        "            course_xml = course.attrib[\"href\"]\n",
        "            # Convert API link → human URL: drop '/cisapp/explorer' and '.xml'\n",
        "            human_url = course_xml.replace(\"/cisapp/explorer\", \"\").rstrip(\".xml\")\n",
        "            course_urls.append(human_url)\n",
        "\n",
        "    return sorted(course_urls)\n",
        "\n",
        "\n",
        "all_urls = extract_uiuc_course_urls(2025, \"fall\")\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "\n",
        "def scrape_uiuc_schedule_selenium(url):\n",
        "    chrome_options = Options()\n",
        "    chrome_options.add_argument(\"--headless\")\n",
        "    chrome_options.add_argument(\"--no-sandbox\")\n",
        "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "\n",
        "    # More verbose error logging\n",
        "    print(f\"Starting to scrape URL: {url}\")\n",
        "\n",
        "    try:\n",
        "        driver = webdriver.Chrome(options=chrome_options)\n",
        "        driver.get(url)\n",
        "\n",
        "        # Wait longer for page to load\n",
        "        wait = WebDriverWait(driver, 20)\n",
        "        wait.until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
        "\n",
        "        # Debugging page load\n",
        "        print(f\"Page title: {driver.title}\")\n",
        "        print(f\"Page URL: {driver.current_url}\")\n",
        "\n",
        "        # Save page source for debugging\n",
        "        with open(\"page_source.html\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(driver.page_source)\n",
        "        print(\"Saved page source to page_source.html\")\n",
        "\n",
        "        # Get basic course information\n",
        "        try:\n",
        "            course_title = driver.find_element(By.CSS_SELECTOR, \"h1\").text.strip()\n",
        "            print(f\"Found course title: {course_title}\")\n",
        "        except NoSuchElementException:\n",
        "            course_title = \"Unknown Course\"\n",
        "            print(\"Could not find course title\")\n",
        "\n",
        "        page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
        "\n",
        "        # Extract description\n",
        "        description = \"\"\n",
        "        desc_match = re.search(r\"Description:([^\\n]+)\", page_text)\n",
        "        if desc_match:\n",
        "            description = desc_match.group(1).strip()\n",
        "            print(f\"Found description: {description[:50]}...\")\n",
        "        else:\n",
        "            print(\"Could not find description\")\n",
        "\n",
        "        # Extract credit hours\n",
        "        credit_hours = \"\"\n",
        "        credit_match = re.search(r\"Credit:\\s*(\\d+(?:\\.\\d+)?)\\s*hours\", page_text)\n",
        "        if credit_match:\n",
        "            credit_hours = credit_match.group(1)\n",
        "            print(f\"Found credit hours: {credit_hours}\")\n",
        "        else:\n",
        "            print(\"Could not find credit hours\")\n",
        "\n",
        "        # Try to expand all details\n",
        "        try:\n",
        "            print(\"Looking for detail expansion buttons...\")\n",
        "            buttons = driver.find_elements(By.TAG_NAME, \"button\")\n",
        "            detail_buttons = [b for b in buttons if \"detail\" in b.text.lower() or \"expand\" in b.text.lower()]\n",
        "\n",
        "            if detail_buttons:\n",
        "                print(f\"Found {len(detail_buttons)} detail buttons, clicking the first one...\")\n",
        "                detail_buttons[0].click()\n",
        "                time.sleep(5)  # Wait longer for details to expand\n",
        "            else:\n",
        "                print(\"No detail buttons found\")\n",
        "\n",
        "                # Try alternate approaches\n",
        "                detail_links = driver.find_elements(By.XPATH, \"//a[contains(text(), 'detail') or contains(text(), 'expand')]\")\n",
        "                if detail_links:\n",
        "                    print(f\"Found {len(detail_links)} detail links, clicking the first one...\")\n",
        "                    detail_links[0].click()\n",
        "                    time.sleep(5)\n",
        "        except Exception as e:\n",
        "            print(f\"Error expanding details: {e}\")\n",
        "\n",
        "        # Find the main course table with multiple approaches\n",
        "        main_table = None\n",
        "\n",
        "        # Try different ways to locate the table\n",
        "        table_approaches = [\n",
        "            # Try to find table with CRN in header\n",
        "            lambda: next((table for table in driver.find_elements(By.TAG_NAME, \"table\")\n",
        "                         if table.find_elements(By.TAG_NAME, \"tr\") and\n",
        "                         \"CRN\" in table.find_element(By.TAG_NAME, \"tr\").text), None),\n",
        "\n",
        "            # Try to find table with specific class\n",
        "            lambda: driver.find_element(By.CSS_SELECTOR, \"table.table\") if driver.find_elements(By.CSS_SELECTOR, \"table.table\") else None,\n",
        "\n",
        "            # Try to find any table\n",
        "            lambda: driver.find_element(By.TAG_NAME, \"table\") if driver.find_elements(By.TAG_NAME, \"table\") else None,\n",
        "\n",
        "            # Try to find table in a specific region\n",
        "            lambda: driver.find_element(By.XPATH, \"//div[contains(@class, 'schedule')]/table\")\n",
        "                   if driver.find_elements(By.XPATH, \"//div[contains(@class, 'schedule')]/table\") else None\n",
        "        ]\n",
        "\n",
        "        for approach in table_approaches:\n",
        "            try:\n",
        "                main_table = approach()\n",
        "                if main_table:\n",
        "                    print(f\"Found table using approach {table_approaches.index(approach) + 1}\")\n",
        "                    break\n",
        "            except Exception as e:\n",
        "                print(f\"Approach failed: {e}\")\n",
        "\n",
        "        if not main_table:\n",
        "            print(\"Could not find main course table\")\n",
        "            # Create a dummy data row with basic course info\n",
        "            return pd.DataFrame([{\n",
        "                \"Description\": description,\n",
        "                \"Credit Hours\": credit_hours,\n",
        "                \"Section Title\": course_title,\n",
        "                \"Section Credit Hours\": credit_hours,\n",
        "                \"CRN\": \"Unknown\",\n",
        "                \"Section\": \"Unknown\",\n",
        "                \"Status Code\": \"\",\n",
        "                \"Part of Term\": \"\",\n",
        "                \"Section Status\": \"\",\n",
        "                \"Enrollment Status\": \"\",\n",
        "                \"Note\": \"Table data not found - this is placeholder data\"\n",
        "            }])\n",
        "\n",
        "        # Count tables for debugging\n",
        "        tables = driver.find_elements(By.TAG_NAME, \"table\")\n",
        "        print(f\"Found {len(tables)} tables on the page\")\n",
        "\n",
        "        # Print the first row of each table for debugging\n",
        "        for i, table in enumerate(tables):\n",
        "            try:\n",
        "                rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
        "                if rows:\n",
        "                    print(f\"Table {i+1} first row: {rows[0].text[:100]}...\")\n",
        "                    # If this is our main table, print more info\n",
        "                    if table == main_table:\n",
        "                        print(f\"  This is our main table with {len(rows)} rows\")\n",
        "                        if len(rows) > 1:\n",
        "                            print(f\"  Second row sample: {rows[1].text[:100]}...\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error examining table {i+1}: {e}\")\n",
        "\n",
        "        data = []\n",
        "        headers = []\n",
        "\n",
        "        try:\n",
        "            header_row = main_table.find_element(By.TAG_NAME, \"tr\")\n",
        "            headers = [th.text.strip() for th in header_row.find_elements(By.TAG_NAME, \"th\")]\n",
        "            print(f\"Found headers: {headers}\")\n",
        "\n",
        "            # If headers are empty, try td elements (some tables use td for headers)\n",
        "            if not headers:\n",
        "                headers = [td.text.strip() for td in header_row.find_elements(By.TAG_NAME, \"td\")]\n",
        "                print(f\"Found headers from td elements: {headers}\")\n",
        "\n",
        "            # If still empty, use default headers\n",
        "            if not headers:\n",
        "                print(\"Using default headers\")\n",
        "                headers = [\"CRN\", \"Status\", \"Section\", \"Type\", \"Time\", \"Day\", \"Location\", \"Instructor\", \"Detail\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting headers: {e}\")\n",
        "            headers = [\"CRN\", \"Status\", \"Section\", \"Type\", \"Time\", \"Day\", \"Location\", \"Instructor\", \"Detail\"]\n",
        "\n",
        "        try:\n",
        "            rows = main_table.find_elements(By.TAG_NAME, \"tr\")[1:]  # Skip header row\n",
        "            print(f\"Processing {len(rows)} data rows\")\n",
        "\n",
        "            for i, row in enumerate(rows):\n",
        "                try:\n",
        "                    cells = row.find_elements(By.TAG_NAME, \"td\")\n",
        "                    if len(cells) < 2:  # Skip rows with too few cells\n",
        "                        print(f\"Skipping row {i+1} - not enough cells ({len(cells)})\")\n",
        "                        continue\n",
        "\n",
        "                    # Initialize the row data with all requested columns\n",
        "                    row_data = {\n",
        "                        \"Description\": description,\n",
        "                        \"Credit Hours\": credit_hours,\n",
        "                        \"Section Title\": course_title,\n",
        "                        \"Section Credit Hours\": credit_hours,  # Default to course credit hours\n",
        "                        \"Section Info\": \"\",\n",
        "                        \"Degree Attributes\": \"\",\n",
        "                        \"Schedule Information\": \"\",\n",
        "                        \"CRN\": \"\",\n",
        "                        \"Section\": \"\",\n",
        "                        \"Status Code\": \"\",\n",
        "                        \"Part of Term\": \"\",\n",
        "                        \"Section Status\": \"\",\n",
        "                        \"Enrollment Status\": \"\",\n",
        "                        \"Type\": \"\",\n",
        "                        \"Type Code\": \"\",\n",
        "                        \"Start Time\": \"\",\n",
        "                        \"End Time\": \"\",\n",
        "                        \"Days of Week\": \"\",\n",
        "                        \"Room\": \"\",\n",
        "                        \"Building\": \"\",\n",
        "                        \"Instructors\": \"\"\n",
        "                    }\n",
        "\n",
        "                    # Add the data from the table cells\n",
        "                    for j, header in enumerate(headers):\n",
        "                        if j < len(cells):\n",
        "                            row_data[header] = cells[j].text.strip()\n",
        "\n",
        "                    # Print first cell for debugging\n",
        "                    if len(cells) > 0:\n",
        "                        print(f\"Row {i+1}, first cell: {cells[0].text.strip()}\")\n",
        "\n",
        "                    # Extract CRN from the table\n",
        "                    if \"CRN\" in row_data and row_data[\"CRN\"]:\n",
        "                        print(f\"Found CRN: {row_data['CRN']}\")\n",
        "                    elif \"CRN\" in headers and len(cells) > headers.index(\"CRN\"):\n",
        "                        row_data[\"CRN\"] = cells[headers.index(\"CRN\")].text.strip()\n",
        "                        print(f\"Extracted CRN from column: {row_data['CRN']}\")\n",
        "\n",
        "                    # Extract Section from the table\n",
        "                    if \"Section\" in row_data and row_data[\"Section\"]:\n",
        "                        pass\n",
        "                    elif \"Section\" in headers and len(cells) > headers.index(\"Section\"):\n",
        "                        row_data[\"Section\"] = cells[headers.index(\"Section\")].text.strip()\n",
        "\n",
        "                    # Extract Status and Status Code\n",
        "                    if \"Status\" in row_data and row_data[\"Status\"]:\n",
        "                        row_data[\"Section Status\"] = row_data[\"Status\"]\n",
        "                        # Try to infer status code\n",
        "                        if row_data[\"Status\"].lower() == \"open\":\n",
        "                            row_data[\"Status Code\"] = \"A\"\n",
        "                        elif row_data[\"Status\"].lower() == \"closed\":\n",
        "                            row_data[\"Status Code\"] = \"C\"\n",
        "\n",
        "                    # Extract Type and Type Code\n",
        "                    if \"Type\" in row_data and row_data[\"Type\"]:\n",
        "                        row_data[\"Type\"] = row_data[\"Type\"]\n",
        "                        row_data[\"Type Code\"] = row_data[\"Type\"]\n",
        "\n",
        "                    # Extract Time and process into Start Time and End Time\n",
        "                    if \"Time\" in row_data and row_data[\"Time\"]:\n",
        "                        time_match = re.search(r\"(\\d+:\\d+[AP]M)\\s*-\\s*(\\d+:\\d+[AP]M)\", row_data[\"Time\"])\n",
        "                        if time_match:\n",
        "                            row_data[\"Start Time\"] = time_match.group(1)\n",
        "                            row_data[\"End Time\"] = time_match.group(2)\n",
        "\n",
        "                    # Extract Days of Week\n",
        "                    if \"Day\" in row_data and row_data[\"Day\"]:\n",
        "                        row_data[\"Days of Week\"] = row_data[\"Day\"]\n",
        "\n",
        "                    # Extract Room and Building from Location\n",
        "                    if \"Location\" in row_data and row_data[\"Location\"]:\n",
        "                        location_parts = row_data[\"Location\"].split(\" \", 1)\n",
        "                        if len(location_parts) >= 2:\n",
        "                            row_data[\"Room\"] = location_parts[0]\n",
        "                            row_data[\"Building\"] = location_parts[1]\n",
        "\n",
        "                    # Extract Instructors\n",
        "                    if \"Instructor\" in row_data and row_data[\"Instructor\"]:\n",
        "                        row_data[\"Instructors\"] = row_data[\"Instructor\"]\n",
        "\n",
        "                    # Extract detailed information from the \"Detail\" column if it exists\n",
        "                    if \"Detail\" in row_data and row_data[\"Detail\"]:\n",
        "                        detail_text = row_data[\"Detail\"]\n",
        "\n",
        "                        # Extract Part of Term\n",
        "                        pot_match = re.search(r\"Part of Term\\s*(\\d+)\", detail_text)\n",
        "                        if pot_match:\n",
        "                            row_data[\"Part of Term\"] = pot_match.group(1).strip()\n",
        "\n",
        "                        # Extract Degree Notes/Attributes\n",
        "                        degree_match = re.search(r\"Degree Notes\\s*(.*?)(?:Date Range|$)\", detail_text)\n",
        "                        if degree_match:\n",
        "                            row_data[\"Degree Attributes\"] = degree_match.group(1).strip()\n",
        "\n",
        "                        # Extract Date Range as Schedule Information\n",
        "                        date_match = re.search(r\"Date Range\\s*Meets\\s*(\\d+/\\d+/\\d+-\\d+/\\d+/\\d+)\", detail_text)\n",
        "                        if date_match:\n",
        "                            row_data[\"Schedule Information\"] = f\"Meets {date_match.group(1)}\"\n",
        "\n",
        "                        # Extract Availability/Enrollment Status\n",
        "                        avail_match = re.search(r\"Availability\\s*(\\w+)\", detail_text)\n",
        "                        if avail_match:\n",
        "                            row_data[\"Enrollment Status\"] = avail_match.group(1).strip()\n",
        "\n",
        "                    # Use the entire Detail as Section Info if nothing better is available\n",
        "                    if \"Detail\" in row_data and not row_data[\"Section Info\"]:\n",
        "                        row_data[\"Section Info\"] = row_data[\"Detail\"]\n",
        "\n",
        "                    data.append(row_data)\n",
        "                    print(f\"Added data row {i+1}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing row {i+1}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing table rows: {e}\")\n",
        "\n",
        "        print(f\"Processed {len(data)} data rows successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Major error: {e}\")\n",
        "        return pd.DataFrame([{\n",
        "            \"Description\": \"Error occurred\",\n",
        "            \"Credit Hours\": \"\",\n",
        "            \"Section Title\": \"\",\n",
        "            \"CRN\": \"\",\n",
        "            \"Error\": str(e)\n",
        "        }])\n",
        "    finally:\n",
        "        try:\n",
        "            driver.quit()\n",
        "            print(\"Driver closed\")\n",
        "        except:\n",
        "            print(\"Error closing driver\")\n",
        "\n",
        "    if not data:\n",
        "        print(\"No data was collected\")\n",
        "        # Return a dataframe with error information\n",
        "        return pd.DataFrame([{\n",
        "            \"Description\": description,\n",
        "            \"Credit Hours\": credit_hours,\n",
        "            \"Section Title\": course_title,\n",
        "            \"CRN\": \"No data found\",\n",
        "            \"Error\": \"Table data could not be extracted\"\n",
        "        }])\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "def test_with_direct_html():\n",
        "    \"\"\"Try to extract info from direct HTML when selenium fails\"\"\"\n",
        "    print(\"Attempting direct HTML extraction as fallback...\")\n",
        "    try:\n",
        "        url = \"https://courses.illinois.edu/schedule/2025/fall/AAS/100\"\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Try to extract basic course info\n",
        "        title = soup.find('h1')\n",
        "        course_title = title.text.strip() if title else \"Unknown Course\"\n",
        "\n",
        "        # Extract description\n",
        "        desc_text = soup.find(text=re.compile(\"Description:\"))\n",
        "        description = \"\"\n",
        "        if desc_text:\n",
        "            parent = desc_text.parent\n",
        "            description = parent.text.replace(\"Description:\", \"\").strip()\n",
        "\n",
        "        # Extract credit hours\n",
        "        credit_text = soup.find(text=re.compile(\"Credit:\"))\n",
        "        credit_hours = \"\"\n",
        "        if credit_text:\n",
        "            credit_match = re.search(r\"Credit:\\s*(\\d+(?:\\.\\d+)?)\\s*hours\", credit_text)\n",
        "            if credit_match:\n",
        "                credit_hours = credit_match.group(1)\n",
        "\n",
        "        # Try to find the course table\n",
        "        tables = soup.find_all('table')\n",
        "\n",
        "        data = []\n",
        "        for table in tables:\n",
        "            headers = [th.text.strip() for th in table.find_all('th')]\n",
        "            if not headers or 'CRN' not in ' '.join(headers):\n",
        "                continue\n",
        "\n",
        "            rows = table.find_all('tr')[1:]  # Skip header row\n",
        "            for row in rows:\n",
        "                cells = row.find_all('td')\n",
        "                if len(cells) < len(headers):\n",
        "                    continue\n",
        "\n",
        "                row_data = {\n",
        "                    \"Description\": description,\n",
        "                    \"Credit Hours\": credit_hours,\n",
        "                    \"Section Title\": course_title,\n",
        "                    \"CRN\": cells[headers.index('CRN')].text.strip() if 'CRN' in headers else \"\",\n",
        "                    \"Type\": cells[headers.index('Type')].text.strip() if 'Type' in headers else \"\",\n",
        "                    \"Section\": cells[headers.index('Section')].text.strip() if 'Section' in headers else \"\",\n",
        "                    \"Degree Attributes\": \"Extracted from HTML fallback\",\n",
        "                    \"Method\": \"HTML_FALLBACK\"\n",
        "                }\n",
        "                data.append(row_data)\n",
        "\n",
        "        if data:\n",
        "            print(f\"HTML fallback found {len(data)} rows\")\n",
        "            return pd.DataFrame(data)\n",
        "        else:\n",
        "            print(\"HTML fallback found no data\")\n",
        "            return pd.DataFrame([{\n",
        "                \"Description\": description,\n",
        "                \"Credit Hours\": credit_hours,\n",
        "                \"Section Title\": course_title,\n",
        "                \"CRN\": \"No data found\",\n",
        "                \"Method\": \"HTML_FALLBACK_EMPTY\"\n",
        "            }])\n",
        "    except Exception as e:\n",
        "        print(f\"HTML fallback error: {e}\")\n",
        "        return pd.DataFrame([{\n",
        "            \"CRN\": \"Error in HTML fallback\",\n",
        "            \"Error\": str(e),\n",
        "            \"Method\": \"HTML_FALLBACK_ERROR\"\n",
        "        }])\n",
        "\n",
        "def main():\n",
        "    print(\"Starting UIUC course scraping...\")\n",
        "    dfs = []\n",
        "    # Try selenium first\n",
        "    #df = scrape_uiuc_schedule_selenium()\n",
        "    for url in all_urls:\n",
        "        print(f\"\\n--- Scraping {url} ---\")\n",
        "        df = scrape_uiuc_schedule_selenium(url)\n",
        "\n",
        "        # Fallback to HTML if selenium gave no useful rows\n",
        "        if df.empty or (all(df['CRN'].astype(str).str.contains('Unknown|Error|No data', case=False))):\n",
        "            print(\"Selenium failed to get data, trying HTML fallback...\")\n",
        "            df = test_with_direct_html()\n",
        "\n",
        "        dfs.append(df)\n",
        "\n",
        "    # Concatenate all results\n",
        "    combined = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Filter to only the requested columns that exist\n",
        "    requested_columns = [\n",
        "        \"Description\", \"Credit Hours\", \"Section Info\", \"Degree Attributes\",\n",
        "        \"Schedule Information\", \"CRN\", \"Section\", \"Status Code\", \"Part of Term\",\n",
        "        \"Section Title\", \"Section Credit Hours\", \"Section Status\", \"Enrollment Status\",\n",
        "        \"Type\", \"Type Code\", \"Start Time\", \"End Time\", \"Days of Week\", \"Room\",\n",
        "        \"Building\", \"Instructors\"\n",
        "    ]\n",
        "    available_columns = [col for col in requested_columns if col in combined.columns]\n",
        "    combined = combined[available_columns] if available_columns else combined\n",
        "\n",
        "    # Save to CSV\n",
        "    output_file = \"uiuc_all_courses.csv\"\n",
        "    combined.to_csv(output_file, index=False)\n",
        "    print(f\"✅ All data saved to {output_file} with {len(combined)} rows and {len(combined.columns)} columns.\")\n",
        "    print(combined.head())\n",
        "\n",
        "\n",
        "    # # If no rows with actual CRN data, try HTML fallback\n",
        "    # if df.empty or (all(df['CRN'].astype(str).str.contains('Unknown|Error|No data', case=False)) if 'CRN' in df.columns else True):\n",
        "    #     print(\"Selenium scraping failed to get data, trying HTML fallback...\")\n",
        "    #     df_html = test_with_direct_html()\n",
        "\n",
        "    #     # If HTML fallback worked, use that data\n",
        "    #     if not df_html.empty and not all(df_html['CRN'].astype(str).str.contains('Unknown|Error|No data', case=False) if 'CRN' in df_html.columns else True):\n",
        "    #         print(\"Using HTML fallback data\")\n",
        "    #         df = df_html\n",
        "    #     else:\n",
        "    #         print(\"Both methods failed\")\n",
        "    #         # Combine error info\n",
        "    #         if not df.empty and not df_html.empty:\n",
        "    #             df = pd.concat([df, df_html], ignore_index=True)\n",
        "\n",
        "    # # Keep only the requested columns that exist in the dataframe\n",
        "    # requested_columns = [\n",
        "    #     \"Description\", \"Credit Hours\", \"Section Info\", \"Degree Attributes\",\n",
        "    #     \"Schedule Information\", \"CRN\", \"Section\", \"Status Code\", \"Part of Term\",\n",
        "    #     \"Section Title\", \"Section Credit Hours\", \"Section Status\", \"Enrollment Status\",\n",
        "    #     \"Type\", \"Type Code\", \"Start Time\", \"End Time\", \"Days of Week\", \"Room\",\n",
        "    #     \"Building\", \"Instructors\"\n",
        "    # ]\n",
        "\n",
        "    # # Filter to only columns that exist in the dataframe\n",
        "    # available_columns = [col for col in requested_columns if col in df.columns]\n",
        "\n",
        "    # if available_columns:\n",
        "    #     df_filtered = df[available_columns]\n",
        "    # else:\n",
        "    #     df_filtered = df\n",
        "\n",
        "\n",
        "\n",
        "    # # Save the output\n",
        "    # output_file = \"uiuc_course_data.csv\"\n",
        "    # df_filtered.to_csv(output_file, index=False)\n",
        "    # print(f\"✅ Data saved to {output_file} with {len(df_filtered)} rows and {len(df_filtered.columns)} columns.\")\n",
        "    # print(df_filtered.head())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeQNaLcx1GtP",
        "outputId": "19e24a0e-54b3-456c-9a25-bb9d08a2bc14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting UIUC course scraping...\n",
            "\n",
            "--- Scraping http://cis.local/cisapi/schedule/2025/fall/AAS/215 ---\n",
            "Starting to scrape URL: http://cis.local/cisapi/schedule/2025/fall/AAS/215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below code processes and cleans a structured CSV file by removing unnecessary rows and extracting key metadata. The result is a cleaner and more usable version of the original dataset."
      ],
      "metadata": {
        "id": "2S4de1YtS55W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "\n",
        "def process_csv(input_file, output_file):\n",
        "    \"\"\"\n",
        "    Process a CSV file by removing alternate rows and extracting 'Part of Term' info\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    processed_rows = []\n",
        "\n",
        "    # Read the input CSV file\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        headers = reader.fieldnames.copy()  # Copy the headers\n",
        "\n",
        "        # Add 'Part of Term' header if it doesn't exist\n",
        "        if 'Part of Term' not in headers:\n",
        "            headers.append('Part of Term')\n",
        "\n",
        "        # Read all rows into memory\n",
        "        for row in reader:\n",
        "            rows.append(row)\n",
        "\n",
        "    # Process rows - keep every other row, extract Part of Term from alternate rows\n",
        "    for i in range(0, len(rows), 2):\n",
        "        if i < len(rows):\n",
        "            current_row = dict(rows[i])  # Copy the current row\n",
        "\n",
        "            # Make sure there's a Part of Term field\n",
        "            if 'Part of Term' not in current_row:\n",
        "                current_row['Part of Term'] = ''\n",
        "\n",
        "            # If we have a next row, look for Part of Term info\n",
        "            if i + 1 < len(rows):\n",
        "                next_row = rows[i + 1]\n",
        "\n",
        "                # Check all fields in the next row for 'Part of Term'\n",
        "                for field, value in next_row.items():\n",
        "                    if value and isinstance(value, str) and 'Part of Term' in value:\n",
        "                        # Extract the Part of Term value (e.g., \"1\" from \"Part of Term 1\")\n",
        "                        match = re.search(r'Part of Term\\s+(\\S+)', value)\n",
        "                        if match:\n",
        "                            current_row['Part of Term'] = match.group(1)\n",
        "                            break\n",
        "\n",
        "            processed_rows.append(current_row)\n",
        "\n",
        "    # Write to output file\n",
        "    with open(output_file, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=headers)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(processed_rows)\n",
        "\n",
        "    print(f\"Successfully processed {len(processed_rows)} rows. Output saved to {output_file}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/uiuc_all_courses.csv\"  # Change this to your input file\n",
        "    output_file = \"courses_processed.csv\"  # Change this to your desired output file\n",
        "\n",
        "    process_csv(input_file, output_file)"
      ],
      "metadata": {
        "id": "xe6WTGfn2Ssg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JrNyMQIwMDMT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}